using POMDPs
using POMDPTools
using Crux
using Flux
1
mdp = convert(MDP, RDEEnv())

as = POMDPs.actions(mdp)
S = ContinuousSpace((mdp.env.prob.params.N*2,))
layer_size = 32
A() = DiscreteNetwork(Chain(Dense(Crux.dim(S)..., layer_size, relu),
                            Dense(layer_size, layer_size, relu),
                            Dense(layer_size, length(as))), as)
V() = ContinuousNetwork(Chain(Dense(Crux.dim(S)..., layer_size, relu),
                              Dense(layer_size, layer_size, relu),
                              Dense(layer_size, 1)))



ğ’®_ppo = PPO(Ï€=ActorCritic(A(), V()), S=S, N=5000, Î”N=200, max_steps=10000)
@time Ï€_ppo = POMDPs.solve(ğ’®_ppo, mdp)



p = plot_learning(ğ’®_ppo)

PolRunData = run_policy(Ï€_ppo, mdp.env;tmax = 26.0)
animate_policy_data(PolRunData, mdp.env; fname="ppo")